{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Softmax model which allows for non-binary outcomes\n",
    "\n",
    "Note: the current version of the Softmax function here uses Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    def __init__(self,eta,nepochs,lam):\n",
    "        self.nepochs = nepochs\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "    \n",
    "    def OneHot(self,y):\n",
    "        hot_mat = np.zeros((len(y), len(np.unique(y))))\n",
    "        for i, val in enumerate(y):\n",
    "            hot_mat[i, val] = 1\n",
    "        return hot_mat\n",
    "        \n",
    "    def net_input(self,x):\n",
    "        return x.dot(self.weights) + self.bias\n",
    "    \n",
    "    def softmax(self,z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T      \n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        self.num_outcomes = len(np.unique(y))\n",
    "        self.weights = np.random.random((len(x[0]),self.num_outcomes))\n",
    "        self.bias = np.random.random(self.num_outcomes)\n",
    "        \n",
    "        y = self.OneHot(y)\n",
    "        \n",
    "        for i in range(self.nepochs):\n",
    "            net_input = self.net_input(x)\n",
    "            softmax = self.softmax(net_input)\n",
    "            error = softmax - y\n",
    "            \n",
    "            grad = np.dot(x.T,error)\n",
    "            \n",
    "            self.weights -= self.eta*(grad + self.lam*self.weights)\n",
    "            self.bias -= self.eta*np.sum(error,axis=0)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self,x):\n",
    "        net_input = self.net_input(x)\n",
    "        softmax = self.softmax(net_input)\n",
    "        return softmax.argmax(axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layered Neural Network (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAB4CAYAAADSWhi9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf6hG7vzmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04306e7be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input of the dataset is the number of pixels in the non-overlapping 4x4 sub-martix of the 32x32 bitmaps of each image generating an 8x8 matrix for each image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soft = Softmax(0.01,10,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Softmax at 0x7f043b117278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "ypred = soft.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
