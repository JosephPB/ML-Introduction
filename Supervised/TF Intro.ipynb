{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn does not offer Neural Network implementation libraries, whereas TensorFlow does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print (iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input function\n",
    "\n",
    "TensorFlow requires features in dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_evaluation(x,y):\n",
    "    \n",
    "    features = {'SepalLength': x[:,0],\n",
    "            'SepalWidth': x[:,1],\n",
    "            'PetalLength': x[:,2],\n",
    "            'PetalWidth': x[:,3]}\n",
    "\n",
    "    labels = y\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "'''batch_size defines the number of data units for batch learning:\n",
    "    batch_size = 1 : online learning\n",
    "    1 < batch_size < len(X) : mini-batch learning\n",
    "    batch_size = len(X) : gradient descent learning'''\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow requires the creation of a dataset and a dataset iterator which provides a way of accessing one dataset element at a time\n",
    "\n",
    "More information here: https://www.tensorflow.org/get_started/datasets_quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \n",
    "    # From_tensor_slices separates the dictionary input into array slices.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle by creating a buffer (here length of 1000 > len(X)), repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Build the Iterator, and return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: a one-shot iterator is the simplest form of iterator, which only iterates once through the dataset and is currently the only type of iterator that is easily useable with an Estimator\n",
    "\n",
    "Other iterators are available which can allow, e.g. more convenient switching between datasets\n",
    "\n",
    "More information here: https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create TF dictionary inputs\n",
    "X_trainTF, y_trainTF = input_evaluation(X_train, y_train)\n",
    "X_testTF, y_testTF = input_evaluation(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Feature columns' is a Google term used to describe the intemetiary structure created to bridge the gap between raw data and the Esitmator input. The user specifies the data type of the raw data and the output data. The feature columns function can be the equivalent of creating one-hot variables for the y values.\n",
    "\n",
    "https://www.tensorflow.org/versions/master/get_started/feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "FEATURE_KEYS = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "feature_columns = [\n",
    "      tf.feature_column.numeric_column(key, shape=1) for key in FEATURE_KEYS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp26_zgmi5\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp26_zgmi5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f41b96d2860>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp26_zgmi5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.70999044, step = 1\n",
      "INFO:tensorflow:global_step/sec: 436.262\n",
      "INFO:tensorflow:loss = 0.021928001, step = 101 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.587\n",
      "INFO:tensorflow:loss = 0.6601869, step = 201 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.643\n",
      "INFO:tensorflow:loss = 0.4384018, step = 301 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 430.641\n",
      "INFO:tensorflow:loss = 0.18581378, step = 401 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 576.299\n",
      "INFO:tensorflow:loss = 0.29081342, step = 501 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.081\n",
      "INFO:tensorflow:loss = 0.0065913643, step = 601 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.15\n",
      "INFO:tensorflow:loss = 0.56136703, step = 701 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.408\n",
      "INFO:tensorflow:loss = 0.003543885, step = 801 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.231\n",
      "INFO:tensorflow:loss = 0.06917374, step = 901 (0.213 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp26_zgmi5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26202643.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f41b96d24e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "classifier.train(input_fn=lambda:train_input_fn(X_trainTF, y_trainTF, batch_size), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-03-17:42:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp26_zgmi5/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-03-17:42:39\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.92105263, average_loss = 0.15918867, global_step = 1000, loss = 0.15918867\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(X_testTF, y_testTF, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow allows for increased dynamism in allowing users to build and alter models which can be used in place of the TensorFlow standard premade Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(X, y,batch_size=1, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def neural_net(x_dict, n_hidden_1, n_hidden_2, num_classes):\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    x = x_dict['images']\n",
    "\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1)\n",
    "\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.layers.dense(layer_2, num_classes)\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    logits = neural_net(features)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
